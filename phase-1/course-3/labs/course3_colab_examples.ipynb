{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcca7c8c",
   "metadata": {},
   "source": [
    "# Course 3: Tiny Neural Network from Scratch\\n\n",
    "A minimal 2-layer network implemented with NumPy for educational purposes. Runs in Colab quickly and shows forward pass and a simple training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aefbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\\n\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bdca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset: two clusters\\n\n",
    "rng = np.random.RandomState(0)\\n\n",
    "n = 200\\n\n",
    "X1 = rng.randn(n,2) + np.array([2,2])\\n\n",
    "X2 = rng.randn(n,2) + np.array([-2,-2])\\n\n",
    "X = np.vstack([X1,X2])\\n\n",
    "y = np.hstack([np.ones(n), np.zeros(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2-layer network\\n\n",
    "def sigmoid(z):\\n\n",
    "    return 1/(1+np.exp(-z))\\n\n",
    "\\n\n",
    "D = 2\\n\n",
    "H = 8\\n\n",
    "W1 = 0.1 * rng.randn(D,H)\\n\n",
    "b1 = np.zeros(H)\\n\n",
    "W2 = 0.1 * rng.randn(H,1)\\n\n",
    "b2 = np.zeros(1)\\n\n",
    "\\n\n",
    "lr = 0.1\\n\n",
    "for epoch in range(200):\\n\n",
    "    # forward\\n\n",
    "    z1 = X.dot(W1) + b1\\n\n",
    "    a1 = np.tanh(z1)\\n\n",
    "    z2 = a1.dot(W2) + b2\\n\n",
    "    yhat = sigmoid(z2).ravel()\\n\n",
    "    # loss (binary cross-entropy)\\n\n",
    "    loss = -np.mean(y * np.log(yhat+1e-8) + (1-y)*np.log(1-yhat+1e-8))\\n\n",
    "    # backprop\\n\n",
    "    dz2 = (yhat - y).reshape(-1,1)/len(y)\\n\n",
    "    dW2 = a1.T.dot(dz2)\\n\n",
    "    db2 = dz2.sum(axis=0)\\n\n",
    "    da1 = dz2.dot(W2.T)\\n\n",
    "    dz1 = da1 * (1 - np.tanh(z1)**2)\\n\n",
    "    dW1 = X.T.dot(dz1)\\n\n",
    "    db1 = dz1.sum(axis=0)\\n\n",
    "    # gradient step\\n\n",
    "    W1 -= lr * dW1\\n\n",
    "    b1 -= lr * db1\\n\n",
    "    W2 -= lr * dW2\\n\n",
    "    b2 -= lr * db2\\n\n",
    "    if epoch % 50 == 0:\\n\n",
    "        print(f'epoch {epoch} loss {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary (coarse)\\n\n",
    "xx, yy = np.meshgrid(np.linspace(-6,6,100), np.linspace(-6,6,100))\\n\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\\n\n",
    "a1g = np.tanh(grid.dot(W1) + b1)\\n\n",
    "zg = sigmoid(a1g.dot(W2) + b2).reshape(xx.shape)\\n\n",
    "plt.contourf(xx, yy, zg, levels=[0,0.5,1], alpha=0.3)\\n\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='k', alpha=0.6)\\n\n",
    "plt.title('Tiny NN decision surface')\\n\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
