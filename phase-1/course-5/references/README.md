## ðŸ“š References for course-5

Below is a curated collection of resources to deepen your understanding of importance sampling, Bayesian inference, and generative modeling. These references span foundational texts, practical tutorials, framework documentation, research papers, and community hubsâ€”mirroring the structure of Course 5â€™s reference folder.

---

### ðŸ“– Fundamental Academic Works  
- **The Master Algorithm** by Pedro Domingos (2015)  
  A broad survey of machine learning paradigms, framing generative vs. discriminative approaches.  
  https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine/dp/0465065706  
- **Deep Learning** by Goodfellow, Bengio & Courville (2016)  
  Chapters on probability, variational methods, and optimization provide theoretical underpinnings for importance sampling.  
  https://www.deeplearningbook.org/  
- **Handwritten Digit Recognition with a Back-Propagation Network** (LeCun et al., 1989)  
  The original CNN paper that illustrates how generative thinking (data augmentation, sampling) improves robustness.  
  https://www.researchgate.net/publication/2437764_Handwritten_Digit_Recognition_with_a_Back-Propagation_Network  

---

### âš™ï¸ Optimization Classics  
- **Adam: A Method for Stochastic Optimization** (Kingma & Ba, 2014)  
  Introduces Adam optimizerâ€”essential for training proposal networks and VAEs.  
  https://arxiv.org/abs/1412.6980  
- **Batch Normalization** (Ioffe & Szegedy, 2015)  
  Stabilizes training, reducing variance in Monte Carlo estimates of network gradients.  
  https://arxiv.org/abs/1502.03167  

---

### ðŸŽ¥ Video Courses & Lectures  
- **Stanford CS231n: Convolutional Neural Networks** (2017)  
  Lectures on optimization and probabilistic interpretations of deep models.  
  https://cs231n.github.io/  
- **Stanford CS224n: NLP with Deep Learning**  
  Covers sequence modeling and sampling-based inference in language models.  
  http://web.stanford.edu/class/cs224n/  
- **DeepLearning.AI TensorFlow Specialization**  
  Practical walkthroughs on TensorFlow probability and sampling layers.  
  https://www.coursera.org/specializations/tensorflow-in-practice  

---

### ðŸ“š Framework Documentation  
- **TensorFlow Guide: Convolutional Neural Networks**  
  Tutorial on building and sampling from probabilistic models with TF 2.x.  
  https://www.tensorflow.org/tutorials/images/cnn  
- **Keras Applications**  
  Pretrained models and examples of using importance weights in transfer learning.  
  https://keras.io/api/applications/  
- **TensorFlow RNN Tutorial**  
  Demonstrates sequence sampling techniques applicable to time-series generative models.  
  https://www.tensorflow.org/guide/keras/rnn  
- **PyTorch Tutorials: Deep Learning with PyTorch**  
  Covers custom sampler implementations and weight normalization in PyTorch.  
  https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html  
- **PyTorch Lightning Documentation**  
  Best practices for scalable sampling-based experiments.  
  https://pytorch-lightning.readthedocs.io/  

---

### ðŸ›  Interactive Platforms & Experiments  
- **TensorFlow Playground**  
  Browser-based toy models to visualize sampling and weight effects.  
  https://playground.tensorflow.org/  
- **ConvNetJS by Karpathy**  
  Train small networks in-browser and experiment with sampling strategies.  
  https://cs.stanford.edu/people/karpathy/convnetjs/  
- **Google Colab Tutorials**  
  Ready-to-run notebooks on probabilistic modeling and importance sampling.  
  https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/  

---

### â˜ Cloud Experimentation  
- **Google Cloud AI Platform**  
  Scalable environment for Monte Carlo and sampling-intensive workloads.  
  https://cloud.google.com/ai-platform  
- **AWS SageMaker**  
  Managed infrastructure for distributed importance sampling experiments.  
  https://aws.amazon.com/sagemaker/  

---

### ðŸ“„ Research Papers & Preprints  
- **AdamW: Decoupled Weight Decay Regularization** (Loshchilov & Hutter, 2017)  
  Improved optimizer for stable sampling in high-dimensional models.  
  https://arxiv.org/abs/1711.05101  
- **AdaBelief Optimizer** (2020)  
  Alternative to Adam offering lower variance in weight updates.  
  https://arxiv.org/abs/2010.07468  
- **Attention Is All You Need** (Vaswani et al., 2017)  
  Transformer architectureâ€”sampling-based generation at the heart of LLMs.  
  https://arxiv.org/abs/1706.03762  

---

### ðŸ¤ Communities & Continuous Learning  
- **r/MachineLearning**  
  Ongoing discussion of Monte Carlo methods and Bayesian inference.  
  https://www.reddit.com/r/MachineLearning/  
- **TensorFlow Forum**  
  Q&A on sampling implementations and probabilistic layers.  
  https://discuss.tensorflow.org/  
- **Papers with Code**  
  Search implementations of importance sampling and generative models.  
  https://paperswithcode.com/area/deep-learning  

---

### ðŸš€ Learning Progression & Career Development  
- **TensorFlow Developer Certificate**  
  Validates hands-on skills in probabilistic modeling and sampling.  
  https://www.tensorflow.org/certificate  
- **AWS Certified Machine Learning â€“ Specialty**  
  Demonstrates expertise in scalable inference and generative techniques.  
  https://aws.amazon.com/certification/certified-machine-learning-specialty/  
- **Distill.pub** & **The Gradient**  
  Interactive explainers and commentary on advanced sampling and generative research.  
  https://distill.pub/  
  https://thegradient.pub/  

---

> This reference collection draws from Course 5â€™s curated resources. Dive into these materials to strengthen your theoretical foundation, explore practical implementations, and join vibrant communities focused on generative AI and probabilistic modeling.
